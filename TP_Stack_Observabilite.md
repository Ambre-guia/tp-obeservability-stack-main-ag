# ğŸ“˜ TP6 : Stack Open-Source d'ObservabilitÃ© (ELK + Jaeger + Prometheus)

**Niveau** : IntermÃ©diaire  
**PrÃ©requis** : Connaissances Docker, Docker Compose, notions de microservices  

---

## ğŸ¯ Objectifs pÃ©dagogiques

Ã€ la fin de ce TP, vous serez capable de :
- DÃ©ployer une stack d'observabilitÃ© complÃ¨te avec Docker Compose
- Comprendre et diffÃ©rencier les 3 piliers : MÃ©triques (Prometheus), Logs (ELK), Traces (Jaeger)
- Configurer Grafana pour crÃ©er des dashboards de monitoring
- Analyser les performances d'une application microservices
- Diagnostiquer des problÃ¨mes en production (latence, erreurs)
- CorrÃ©ler logs, mÃ©triques et traces pour du troubleshooting avancÃ©

---

## ğŸ“‹ Contexte du projet

Vous allez dÃ©ployer **MesProduits Observability Stack**, une application microservices complÃ¨te comprenant :

| Composant | Description | Port |
|-----------|-------------|------|
| **Frontend (Node.js)** | Interface web avec mÃ©triques exposÃ©es | 3000 |
| **Backend (Flask)** | API REST avec logs structurÃ©s et tracing | 5002 |
| **PostgreSQL** | Base de donnÃ©es avec exporter de mÃ©triques | 5433 |
| **Prometheus** | Collecte et stockage des mÃ©triques | 9091 |
| **Grafana** | Visualisation et dashboards | 3001 |
| **Elasticsearch** | Stockage et indexation des logs | 9200 |
| **Logstash** | AgrÃ©gation et transformation des logs | 5001 |
| **Kibana** | Exploration et analyse des logs | 5601 |
| **Jaeger** | Distributed tracing | 16686 |
| **Postgres Exporter** | MÃ©triques PostgreSQL | 9187 |

L'objectif est de monitorer l'application complÃ¨te et diagnostiquer des incidents simulÃ©s.

---

## â“ QUESTIONS PRÃ‰LIMINAIRES (Ã€ rÃ©pondre avant de commencer)

### Section A : ComprÃ©hension thÃ©orique

**Q1.** Citez les 3 piliers de l'observabilitÃ© et donnez un exemple d'outil pour chacun.

Votre rÃ©ponse :
- Pilier 1 : _________________________ â†’ Outil : _________________________
- Pilier 2 : _________________________ â†’ Outil : _________________________
- Pilier 3 : _________________________ â†’ Outil : _________________________

---

**Q2.** Quelle est la diffÃ©rence fondamentale entre monitoring et observabilitÃ© ?

Votre rÃ©ponse :
```
________________________________________________________________________
________________________________________________________________________
________________________________________________________________________
```

---

**Q3.** Associez chaque cas d'usage Ã  l'outil appropriÃ© :

| Cas d'usage | Outil (Prometheus/ELK/Jaeger) |
|-------------|-------------------------------|
| Analyser pourquoi une requÃªte met 5 secondes | _________ |
| Voir le taux de requÃªtes HTTP par seconde | _________ |
| Chercher les erreurs 500 dans les logs | _________ |
| Identifier quel microservice cause la latence | _________ |

---

**Q4.** Qu'est-ce qu'un Golden Signal en SRE ? Citez les 4 types.

Votre rÃ©ponse :
```
1. _________________________
2. _________________________
3. _________________________
4. _________________________
```

---

**Q5.** Expliquez la diffÃ©rence entre push et pull pour la collecte de mÃ©triques.

Votre rÃ©ponse :
```
________________________________________________________________________
________________________________________________________________________
________________________________________________________________________
```

---

## ğŸ›  Partie 1 : DÃ©ploiement de la stack

### Ã‰tape 1.1 : RÃ©cupÃ©ration du projet

La structure du projet est la suivante :

```
tp6-observability-stack/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .env
â”œâ”€â”€ README.md
â”œâ”€â”€ QUICKSTART.md
â”œâ”€â”€ init.sql
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ server.js
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ index.html
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ init_db.py
â”‚
â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ prometheus.yml
â”‚
â”œâ”€â”€ logstash/
â”‚   â””â”€â”€ pipeline.conf
â”‚
â””â”€â”€ grafana/
    â””â”€â”€ provisioning/
        â”œâ”€â”€ datasources/
        â”‚   â””â”€â”€ datasources.yml
        â””â”€â”€ dashboards/
            â””â”€â”€ dashboards.yml
```

---

### Ã‰tape 1.2 : Configuration de l'environnement

Le fichier `.env` contient dÃ©jÃ  toutes les valeurs par dÃ©faut :

```bash
# PostgreSQL
POSTGRES_USER=user
POSTGRES_PASSWORD=pass
POSTGRES_DB=products

# Grafana
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin
```

**Q6.** Ces valeurs sont-elles adaptÃ©es pour un environnement de production ?

Votre rÃ©ponse : â˜ Oui â˜ Non

Si non, pourquoi ? ___________________________________________________________

---

### Ã‰tape 1.3 : DÃ©marrage de la stack

Lancez tous les services :

```bash
docker compose up -d
```

**Q7.** Quelle commande permet de vÃ©rifier que tous les conteneurs sont bien dÃ©marrÃ©s et en Ã©tat "healthy" ?

Commande : `docker compose __________`

---

**Q8.** Combien de conteneurs doivent Ãªtre actifs au total ?

RÃ©ponse : **_______** conteneurs

---

### Ã‰tape 1.4 : VÃ©rification des services

ComplÃ©tez le tableau en accÃ©dant Ã  chaque interface :

| Service | URL | Credentials | Status âœ…/âŒ |
|---------|-----|-------------|-------------|
| Frontend | http://localhost:3000 | - | |
| Backend API | http://localhost:5002/health | - | |
| Prometheus | http://localhost:_____ | - | |
| Grafana | http://localhost:_____ | admin/admin | |
| Kibana | http://localhost:_____ | - | |
| Jaeger UI | http://localhost:_____ | - | |

---

**Q9.** Prenez un screenshot de la page d'accueil du Frontend montrant les boutons de test.

ğŸ“¸ **Screenshot Ã  inclure** : `screenshot_frontend.png`

---

**Q10.** ExÃ©cutez la commande suivante et analysez les logs :

```bash
docker compose logs backend --tail=20 | grep "initialisÃ©"
```

Copiez la ligne indiquant que l'application Flask est initialisÃ©e :

```
________________________________________________________________________
```

---

## âœ… Checkpoint 1

- â˜ Tous les conteneurs dÃ©marrÃ©s
- â˜ AccÃ¨s aux 6 interfaces web validÃ©
- â˜ Questions Q1 Ã  Q10 rÃ©pondues

---

## ğŸ“Š Partie 2 : MÃ©triques avec Prometheus & Grafana

### Ã‰tape 2.1 : Exploration de Prometheus

Ouvrez Prometheus : **http://localhost:9091**

**Q11.** Allez dans **Status > Targets**. Combien de targets sont scrapÃ©es par Prometheus ? Listez-les.

RÃ©ponse :
```
Target 1 : _____________________
Target 2 : _____________________
Target 3 : _____________________
Target 4 : _____________________
Target 5 : _____________________
```

---

**Q12.** Toutes les targets sont-elles en Ã©tat **UP** (verte) ? Si une target est DOWN, quelle pourrait Ãªtre la cause ?

Votre rÃ©ponse :
```
________________________________________________________________________
________________________________________________________________________
```

---

### Ã‰tape 2.2 : RequÃªtes PromQL

**Q13.** Dans l'onglet **Graph**, exÃ©cutez cette requÃªte et notez le rÃ©sultat :

```promql
up
```

Que signifie un rÃ©sultat `up{job="frontend"} = 1` ?

Votre rÃ©ponse : _____________________________________________________________

---

**Q14.** ExÃ©cutez cette requÃªte pour voir le taux de requÃªtes HTTP par seconde sur les 5 derniÃ¨res minutes :

```promql
rate(http_requests_total[5m])
```

Combien de sÃ©ries temporelles sont retournÃ©es ? **__________**

---

**Q15.** Filtrez pour ne voir que les requÃªtes vers le backend avec le code 200 :

```promql
http_requests_total{job="________", status="________"}
```

---

### Ã‰tape 2.3 : GÃ©nÃ©ration de trafic

Ouvrez un terminal et exÃ©cutez :

```bash
# GÃ©nÃ©rer 20 requÃªtes normales
for i in {1..20}; do curl http://localhost:3000/api/products; sleep 1; done
```

**Q16.** Pendant que le script tourne, exÃ©cutez dans Prometheus :

```promql
sum(rate(http_requests_total[1m]))
```

Quelle valeur approximative obtenez-vous (req/s) ? **__________**

---

### Ã‰tape 2.4 : Analyse de latence

**Q17.** Le backend expose une mÃ©trique `http_request_duration_seconds` (histogram). Quelle requÃªte PromQL permet de calculer la latence mÃ©diane (P50) sur 5 minutes ?

```promql
histogram_quantile(_______, 
  rate(http_request_duration_seconds_bucket[5m])
)
```

---

**Q18.** Dans l'interface web du frontend (http://localhost:3000), cliquez 5 fois sur le bouton **"RequÃªte Lente"**.

Quelle est la nouvelle latence P50 dans Prometheus ? **__________** secondes

---

**Q19.** Comparez avec la latence P99 :

```promql
histogram_quantile(0.99, 
  rate(http_request_duration_seconds_bucket[5m])
)
```

P99 = **__________** secondes

---

**Q20.** Que signifie une diffÃ©rence importante entre P50 et P99 ?

Votre rÃ©ponse :
```
________________________________________________________________________
________________________________________________________________________
```

---

### Ã‰tape 2.5 : Dashboard Grafana

Ouvrez Grafana : **http://localhost:3001** (admin/admin)

**Q21.** Allez dans **Configuration > Data Sources**. Quel est le status de la datasource Prometheus ?

- â˜ Connected âœ…
- â˜ Error âŒ

---

**Q22.** CrÃ©ez un nouveau dashboard :

1. Cliquez sur **+ > Dashboard > Add visualization**
2. SÃ©lectionnez la datasource **Prometheus**
3. Utilisez cette requÃªte :

```promql
sum(rate(http_requests_total[5m])) by (status)
```

4. Choisissez le type de visualisation **Time series**
5. Titre : "Taux de requÃªtes par status"

Prenez un screenshot.

ğŸ“¸ **Screenshot Ã  inclure** : `screenshot_grafana_dashboard.png`

---

**Q23.** Ajoutez un second panneau montrant le taux d'erreur (% de 5xx) :

```promql
sum(rate(http_requests_total{status=~"5.."}[5m])) 
/ 
sum(rate(http_requests_total[5m])) * 100
```

Quelle est la valeur actuelle ? **__________** %

---

**Q24.** Sauvegardez votre dashboard et exportez-le au format JSON.

Chemin : **Dashboard settings > __________ > __________**

---

## âœ… Checkpoint 2

- â˜ Prometheus targets validÃ©es
- â˜ 5 requÃªtes PromQL exÃ©cutÃ©es
- â˜ Dashboard Grafana crÃ©Ã© avec 2 panneaux
- â˜ Questions Q11 Ã  Q24 rÃ©pondues

---

## ğŸ“‹ Partie 3 : Logs avec ELK Stack

### Ã‰tape 3.1 : Configuration de Kibana

Ouvrez Kibana : **http://localhost:5601**

**Q25.** Ã€ la premiÃ¨re connexion, crÃ©ez un Index Pattern :

1. Allez dans **Stack Management > Index Patterns** (â˜° menu > Management)
2. Cliquez sur **Create index pattern**
3. Index pattern : `logs-*`
4. Time field : `@timestamp`

Combien d'index Elasticsearch correspondent Ã  ce pattern ? **__________**

**Indice** : Allez dans **Dev Tools** et exÃ©cutez :

```
GET _cat/indices/logs-*?v
```

---

**Q26.** Si aucun index n'existe encore, attendez 2-3 minutes que Logstash ingÃ¨re les premiers logs, puis gÃ©nÃ©rez du trafic :

```bash
for i in {1..10}; do curl http://localhost:3000/api/products; sleep 1; done
```

---

### Ã‰tape 3.2 : Exploration des logs

Allez dans **Discover**.

**Q27.** Filtrez pour n'afficher que les logs du backend en utilisant KQL (Kibana Query Language) :

```
service_name : "__________"
```

Combien de logs sont affichÃ©s sur les 15 derniÃ¨res minutes ? **__________**

---

**Q28.** Ajoutez un filtre pour ne voir que les logs de niveau ERROR :

```
service_name : "backend" AND log_level : "__________"
```

---

**Q29.** Cliquez sur un log et dÃ©veloppez le JSON. Quels champs sont prÃ©sents ? (citez-en 5)

```
1. _________________________
2. _________________________
3. _________________________
4. _________________________
5. _________________________
```

---

**Q30.** RepÃ©rez le champ `trace_id` dans un log. Copiez sa valeur ici :

```
trace_id : _____________________________________________________
```

---

### Ã‰tape 3.3 : GÃ©nÃ©ration d'erreurs

Dans l'interface web (http://localhost:3000), cliquez 10 fois sur le bouton **"GÃ©nÃ©rer Erreur"**.

**Q31.** RafraÃ®chissez Kibana. Combien de nouveaux logs ERROR apparaissent ? **__________**

---

**Q32.** CrÃ©ez une visualisation montrant le nombre de logs par niveau :

1. Allez dans **Visualize Library > Create visualization**
2. Choisissez **Vertical Bar**
3. Select index pattern : `logs-*`
4. **Buckets** :
   - X-axis : Aggregation = **Terms**, Field = `log_level.keyword`
5. **Metrics** :
   - Y-axis : Aggregation = **Count**

Prenez un screenshot.

ğŸ“¸ **Screenshot Ã  inclure** : `screenshot_kibana_logs_chart.png`

---

### Ã‰tape 3.4 : Analyse des logs structurÃ©s

**Q33.** Le backend envoie des logs au format JSON. Quel est l'avantage par rapport Ã  des logs en texte libre ?

Votre rÃ©ponse :
```
________________________________________________________________________
________________________________________________________________________
```

---

**Q34.** Ouvrez le fichier `logstash/pipeline.conf`. Quel filtre est utilisÃ© pour parser les logs JSON ?

Ligne : `__________ { source => "message" }`

---

**Q35.** Dans Kibana, crÃ©ez un filtre pour afficher uniquement les requÃªtes vers `/api/slow` :

```
log_message : "*slow*"
```

Combien de rÃ©sultats ? **__________**

---

## âœ… Checkpoint 3

- â˜ Index pattern Kibana crÃ©Ã©
- â˜ Logs filtrÃ©s par service et niveau
- â˜ Visualisation crÃ©Ã©e
- â˜ Questions Q25 Ã  Q35 rÃ©pondues

---

## ğŸ” Partie 4 : Distributed Tracing avec Jaeger

### Ã‰tape 4.1 : Exploration de Jaeger

Ouvrez Jaeger : **http://localhost:16686**

**Q36.** Dans le menu **Search**, sÃ©lectionnez le service **frontend-service** et cliquez sur **Find Traces**.

Combien de traces sont affichÃ©es sur les 15 derniÃ¨res minutes ? **__________** traces

---

**Q37.** Cliquez sur une trace avec une durÃ©e < 100ms. Combien de spans compose cette trace ?

RÃ©ponse : **__________** spans

---

**Q38.** Listez les spans dans l'ordre d'exÃ©cution :

```
1. _________________________
2. _________________________
3. _________________________
```

---

### Ã‰tape 4.2 : Analyse d'une requÃªte lente

Dans l'interface web, cliquez sur le bouton **"RequÃªte Lente"**.

**Q39.** Dans Jaeger, filtrez les traces par **Min Duration = 5s**. SÃ©lectionnez la trace la plus rÃ©cente.

DurÃ©e totale : **__________** ms

---

**Q40.** Quel span est responsable de la latence ? (identifiez l'opÃ©ration et sa durÃ©e)

```
OpÃ©ration : _____________________
DurÃ©e : __________ ms
```

---

**Q41.** Cliquez sur le span lent et allez dans l'onglet **Tags**. Quelle est la valeur du tag `delay.seconds` ?

```
delay.seconds : __________
```

---

**Q42.** Copiez le **Trace ID** de cette trace :

```
Trace ID : _____________________________________________________
```

---

### Ã‰tape 4.3 : CorrÃ©lation Logs â†” Traces

**Q43.** Retournez dans Kibana et cherchez ce `trace_id` dans les logs :

```
trace_id : "VOTRE_TRACE_ID_ICI"
```

Combien de logs correspondent ? **__________**

---

**Q44.** Prenez un screenshot montrant Jaeger et Kibana cÃ´te Ã  cÃ´te avec le mÃªme `trace_id`.

ğŸ“¸ **Screenshot Ã  inclure** : `screenshot_correlation_logs_traces.png`

---

### Ã‰tape 4.4 : Comparaison des traces

**Q45.** Comparez ces deux requÃªtes dans l'interface web :

- RequÃªte A : Cliquez sur **"RequÃªte Normale"**
- RequÃªte B : Cliquez sur **"RequÃªte Lente"**

ComplÃ©tez le tableau en consultant Jaeger :

| MÃ©trique | RequÃªte A | RequÃªte B |
|----------|-----------|-----------|
| DurÃ©e totale | ______ ms | ______ ms |
| Nombre de spans | ______ | ______ |
| Span le plus lent | ______ | ______ |

---

**Q46.** Dans Jaeger, sÃ©lectionnez les deux traces et utilisez le bouton **Compare**. Quelle diffÃ©rence majeure observez-vous ?

Votre rÃ©ponse :
```
________________________________________________________________________
```

---

**Q47.** Le backend utilise la bibliothÃ¨que `jaeger-client` pour le tracing. Quels headers HTTP propagent le contexte de trace entre frontend et backend ?

- â˜ X-Trace-Id
- â˜ uber-trace-id
- â˜ X-Request-Id
- â˜ traceparent

---

## âœ… Checkpoint 4

- â˜ Traces explorÃ©es dans Jaeger
- â˜ RequÃªte lente analysÃ©e
- â˜ CorrÃ©lation logs â†” traces effectuÃ©e
- â˜ Questions Q36 Ã  Q47 rÃ©pondues

---

## ğŸš¨ Partie 5 : Diagnostic d'incident

### ScÃ©nario : Le backend renvoie des erreurs 500

Vous recevez une alerte : **"Augmentation soudaine des erreurs 500 sur le backend depuis 5 minutes"**.

Votre mission : diagnostiquer la cause en utilisant les 3 piliers de l'observabilitÃ©.

---

### Ã‰tape 5.1 : MÃ©triques - Quantifier le problÃ¨me

GÃ©nÃ©rez des erreurs :

```bash
# Dans l'interface web, cliquez 20 fois sur "GÃ©nÃ©rer Erreur"
```

**Q48.** Dans Prometheus, calculez le taux d'erreur 5xx sur les 5 derniÃ¨res minutes :

```promql
sum(rate(http_requests_total{status=~"5.."}[5m])) 
/ 
sum(rate(http_requests_total[5m])) * 100
```

Taux d'erreur actuel : **__________** %

---

**Q49.** Identifiez quel endpoint est le plus impactÃ© :

```promql
sum(rate(http_requests_total{status=~"5.."}[5m])) by (path)
```

Endpoint problÃ©matique : **__________**

---

### Ã‰tape 5.2 : Logs - Identifier la cause

**Q50.** Dans Kibana, cherchez les logs ERROR du backend sur les 5 derniÃ¨res minutes :

```
service_name : "backend" AND log_level : "ERROR" AND @timestamp >= now-5m
```

Quelle est l'erreur la plus frÃ©quente ? (copiez le message)

```
Error message : _____________________________________________________
```

---

**Q51.** Analysez un log d'erreur. Quels champs vous donnent des indices sur la cause ?

Champs utiles :
```
1. _________________________
2. _________________________
3. _________________________
```

---

### Ã‰tape 5.3 : Traces - Localiser le bottleneck

**Q52.** Dans Jaeger, filtrez les traces avec **Tags = error:true**.

Combien de traces en erreur ? **__________**

---

**Q53.** SÃ©lectionnez une trace en erreur. Quel span a un tag `error` ?

```
Span en erreur : _____________________
Tag error.kind : _____________________
```

---

**Q54.** Examinez les logs du span. Quelle est la cause probable de l'erreur ?

- â˜ Timeout de connexion Ã  la base de donnÃ©es
- â˜ Erreur intentionnelle gÃ©nÃ©rÃ©e pour le test
- â˜ Contrainte de clÃ© Ã©trangÃ¨re violÃ©e
- â˜ Table inexistante

---

### Ã‰tape 5.4 : SynthÃ¨se du diagnostic

**Q55.** En vous basant sur vos observations (mÃ©triques + logs + traces), proposez une hypothÃ¨se sur la cause racine de l'incident :

Votre hypothÃ¨se :
```
________________________________________________________________________
________________________________________________________________________
________________________________________________________________________
```

---

**Q56.** Proposez 2 actions correctives :

```
1. _____________________________________________________________________

2. _____________________________________________________________________
```

---

**Q57.** CrÃ©ez un dashboard d'incident dans Grafana avec :

1. **Panneau 1** : Taux d'erreur 5xx
```promql
sum(rate(http_requests_total{status=~"5.."}[5m])) by (path)
```

2. **Panneau 2** : Latence P99
```promql
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

3. **Panneau 3** : Nombre d'erreurs frontend
```promql
sum(frontend_errors_total) by (type)
```

Prenez un screenshot.

ğŸ“¸ **Screenshot Ã  inclure** : `screenshot_incident_dashboard.png`

---

## âœ… Checkpoint 5 (BONUS)

- â˜ Taux d'erreur quantifiÃ©
- â˜ Logs d'erreur analysÃ©s
- â˜ Traces en erreur identifiÃ©es
- â˜ Diagnostic complet rÃ©digÃ©
- â˜ Dashboard d'incident crÃ©Ã©

---

## ğŸ“¦ Livrables

Vous devez rendre un document PDF contenant :

âœ… Page de garde avec nom, prÃ©nom, date  
âœ… RÃ©ponses aux **57 questions** (Q1 Ã  Q57)  
âœ… **7 screenshots** :
- `screenshot_frontend.png`
- `screenshot_grafana_dashboard.png`
- `screenshot_kibana_logs_chart.png`
- `screenshot_correlation_logs_traces.png`
- `screenshot_incident_dashboard.png` (bonus)

âœ… Export JSON du dashboard Grafana (en annexe)  
âœ… Commandes Docker utilisÃ©es (`docker compose ps`, logs, etc.)

**Format** : `TP_NOM_Prenom.pdf`

---


## ğŸ“š Annexes

### Annexe A : Aide-mÃ©moire PromQL

```promql
# MÃ©trique brute
http_requests_total

# Filtrage par label
http_requests_total{job="backend", status="200"}

# Taux par seconde (5 min)
rate(http_requests_total[5m])

# Somme agrÃ©gÃ©e
sum(rate(http_requests_total[5m])) by (status)

# Latence P50
histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))

# Latence P99
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# Taux d'erreur (%)
sum(rate(http_requests_total{status=~"5.."}[5m])) 
/ 
sum(rate(http_requests_total[5m])) * 100
```

---

### Annexe B : Aide-mÃ©moire Kibana Query Language (KQL)

```
# Recherche simple
service_name : "backend"

# ET logique
service_name : "backend" AND log_level : "ERROR"

# OU logique
log_level : "ERROR" OR log_level : "WARN"

# Intervalle de temps
@timestamp >= now-15m

# Wildcard
log_message : "/api/*"

# Existence d'un champ
trace_id : *

# Valeur numÃ©rique
status >= 500
```

---

### Annexe C : Commandes Docker Compose utiles

```bash
# DÃ©marrer la stack
docker compose up -d

# Voir les logs d'un service
docker compose logs -f backend

# Voir l'Ã©tat des services
docker compose ps

# RedÃ©marrer un service
docker compose restart prometheus

# ArrÃªter la stack
docker compose down

# Supprimer les volumes (âš ï¸ efface les donnÃ©es)
docker compose down -v

# Voir les ressources consommÃ©es
docker stats
```

---

### Annexe D : URLs de la stack

| Service | URL | Credentials |
|---------|-----|-------------|
| Frontend | http://localhost:3000 | - |
| Backend | http://localhost:5002 | - |
| Prometheus | http://localhost:9091 | - |
| Grafana | http://localhost:3001 | admin/admin |
| Kibana | http://localhost:5601 | - |
| Jaeger | http://localhost:16686 | - |
| Elasticsearch | http://localhost:9200 | - |
| PostgreSQL | localhost:5433 | user/pass |

---

### Annexe E : Troubleshooting

#### âŒ Elasticsearch ne dÃ©marre pas

**Erreur** : `max virtual memory areas vm.max_map_count [65530] is too low`

**Solution Linux** :
```bash
sudo sysctl -w vm.max_map_count=262144
```

**Solution macOS/Windows** (Docker Desktop) :
- ParamÃ¨tres Docker Desktop
- Resources > Advanced > Memory : min 4 Go

---

#### âŒ Grafana : "Datasource not found"

**Solution** :
1. VÃ©rifiez que Prometheus est UP : `docker compose ps`
2. VÃ©rifiez l'URL dans Grafana : `http://prometheus:9090`
3. Testez : Configuration > Data Sources > Prometheus > Test

---

#### âŒ Jaeger : Aucune trace affichÃ©e

**Causes possibles** :
- Le frontend/backend n'envoient pas de traces
- Jaeger agent non accessible

**Debug** :
```bash
docker compose logs backend | grep -i "jaeger\|trace"
```

---

#### âŒ Kibana : "Index pattern creation failed"

**Solution** :
1. VÃ©rifier qu'Elasticsearch a des donnÃ©es :
```bash
curl http://localhost:9200/_cat/indices?v
```

2. Attendre 2-3 minutes que Logstash ingÃ¨re les logs
3. GÃ©nÃ©rer du trafic sur l'application
4. RafraÃ®chir le pattern

---

## ğŸŒ Ressources complÃ©mentaires

- ğŸ“– [Prometheus Documentation](https://prometheus.io/docs/)
- ğŸ“– [Grafana Dashboard Guide](https://grafana.com/docs/grafana/latest/dashboards/)
- ğŸ“– [Elastic Stack Documentation](https://www.elastic.co/guide/index.html)
- ğŸ“– [Jaeger Documentation](https://www.jaegertracing.io/docs/)
- ğŸ“– [OpenTelemetry](https://opentelemetry.io/)
- ğŸ“– [Google SRE Book - Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)

---

## ğŸ’¡ Conseils

> **L'observabilitÃ© n'est pas du monitoring** - c'est la capacitÃ© Ã  comprendre l'Ã©tat interne d'un systÃ¨me en observant ses outputs. ğŸ”

**Astuce** : Sauvegardez rÃ©guliÃ¨rement vos screenshots et rÃ©ponses au fur et Ã  mesure du TP !

---

**Bon travail ! ğŸ’ª**
