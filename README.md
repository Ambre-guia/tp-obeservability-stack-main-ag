# ğŸš€ Stack d'ObservabilitÃ© ComplÃ¨te

Stack complÃ¨te d'observabilitÃ© pour microservices avec **monitoring**, **logging** et **tracing distribuÃ©**.

## ğŸ“‹ Architecture de la Stack

### Microservices
- **Frontend** (Node.js/Express) - Interface web + API Gateway
- **Backend** (Python/Flask) - API REST avec PostgreSQL
- **Database** (PostgreSQL 15) - Stockage des donnÃ©es

### ObservabilitÃ©
- **Prometheus** - Collecte et stockage des mÃ©triques
- **Grafana** - Visualisation des mÃ©triques et dashboards
- **Elasticsearch** - Stockage et indexation des logs
- **Logstash** - AgrÃ©gation et transformation des logs
- **Kibana** - Exploration et analyse des logs
- **Jaeger** - Tracing distribuÃ© des requÃªtes
- **Postgres Exporter** - MÃ©triques PostgreSQL

## ğŸ¯ Les 3 Piliers de l'ObservabilitÃ©

### 1. ğŸ“Š MÃ©triques (Prometheus + Grafana)
- Collecte automatique toutes les 15s
- MÃ©triques applicatives : requÃªtes HTTP, latence, erreurs
- MÃ©triques systÃ¨me : CPU, mÃ©moire, disque
- MÃ©triques DB : connexions, requÃªtes, performances

### 2. ğŸ“ Logs (ELK Stack)
- Logs JSON structurÃ©s
- Indexation et recherche rapide
- CorrÃ©lation avec les traces via `trace_id`
- Retention configurable

### 3. ğŸ” Traces (Jaeger)
- TraÃ§age des requÃªtes frontend â†’ backend
- Spans pour chaque opÃ©ration (HTTP, SQL)
- Visualisation du flow complet
- DÃ©tection des goulots d'Ã©tranglement

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis
- Docker 20.10+
- Docker Compose 2.0+
- 8 GB RAM minimum
- 10 GB espace disque

### Installation

```bash
# Cloner ou se placer dans le rÃ©pertoire
cd tp6-observability-stack

# Copier le fichier d'environnement
cp .env.example .env

# Ã‰diter les variables si nÃ©cessaire
nano .env

# Construire et dÃ©marrer tous les services
docker-compose up -d

# Voir les logs de tous les services
docker-compose logs -f

# Voir les logs d'un service spÃ©cifique
docker-compose logs -f backend
```

### VÃ©rification du dÃ©marrage

```bash
# VÃ©rifier que tous les services sont UP
docker-compose ps

# VÃ©rifier les healthchecks
docker-compose ps | grep -i healthy
```

## ğŸŒ URLs d'accÃ¨s

| Service | URL | Credentials | Description |
|---------|-----|------------|-------------|
| **Frontend** | http://localhost:3000 | - | Interface web de test |
| **Backend API** | http://localhost:5000 | - | API REST |
| **Grafana** | http://localhost:3001 | admin / admin | Dashboards mÃ©triques |
| **Prometheus** | http://localhost:9090 | - | Interface Prometheus |
| **Kibana** | http://localhost:5601 | - | Exploration logs |
| **Jaeger** | http://localhost:16686 | - | Visualisation traces |
| **Elasticsearch** | http://localhost:9200 | - | API Elasticsearch |
| **PostgreSQL** | localhost:5432 | user / pass | Base de donnÃ©es |

## ğŸ“Š Guide d'utilisation

### 1. Tester l'application

Ouvrez http://localhost:3000 et utilisez les boutons :
- **RequÃªte Normale** : Charge la liste des produits
- **RequÃªte Lente** : Simule une latence de 5s
- **GÃ©nÃ©rer Erreur** : DÃ©clenche une erreur 500
- **Charger 100 RequÃªtes** : Test de charge

### 2. Visualiser les mÃ©triques (Grafana)

1. Ouvrir http://localhost:3001 (admin/admin)
2. Aller dans **Explore** ou **Dashboards**
3. SÃ©lectionner la datasource **Prometheus**
4. Exemples de requÃªtes :

```promql
# RequÃªtes par seconde
rate(http_requests_total[5m])

# Latence P95
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Taux d'erreurs
rate(http_requests_total{status=~"5.."}[5m])

# MÃ©triques PostgreSQL
pg_stat_database_tup_fetched{datname="products"}
```

### 3. Explorer les logs (Kibana)

1. Ouvrir http://localhost:5601
2. Aller dans **Management** â†’ **Index Patterns**
3. CrÃ©er un index pattern : `logs-*` avec champ temps `@timestamp`
4. Aller dans **Discover** pour explorer les logs
5. Exemples de recherches :

```
service_name:"backend" AND log_level:"ERROR"
trace_id:"abc123def456"
log_message:*product*
```

### 4. Analyser les traces (Jaeger)

1. Ouvrir http://localhost:16686
2. SÃ©lectionner le service : **frontend-service** ou **backend-service**
3. Filtrer par opÃ©ration : `GET /api/products`, etc.
4. Cliquer sur une trace pour voir :
   - DurÃ©e totale
   - Spans individuels
   - Tags et logs
   - Propagation frontend â†’ backend

### 5. CorrÃ©lation Logs â†” Traces

Les logs contiennent le `trace_id` de Jaeger :

```json
{
  "service": "backend",
  "message": "RÃ©cupÃ©ration de tous les produits",
  "trace_id": "1234567890abcdef",
  "span_id": "abcdef1234567890"
}
```

Dans Kibana, filtrer par `trace_id` pour voir tous les logs d'une requÃªte.
Copier le `trace_id` et le chercher dans Jaeger pour voir la trace complÃ¨te.

## ğŸ“¡ API Endpoints

### Frontend (Port 3000)
```bash
GET  /                  # Interface web
GET  /health           # Healthcheck
GET  /metrics          # MÃ©triques Prometheus
GET  /api/products     # Liste produits (via backend)
GET  /api/slow         # Endpoint lent
GET  /api/error        # GÃ©nÃ¨re erreur
```

### Backend (Port 5000)
```bash
GET  /health           # Healthcheck + DB status
GET  /metrics          # MÃ©triques Prometheus
GET  /products         # Liste tous les produits
GET  /products/:id     # DÃ©tail d'un produit
POST /products         # CrÃ©er un produit
GET  /slow             # Simule latence 5s
GET  /error            # GÃ©nÃ¨re erreur alÃ©atoire
```

### Exemples avec cURL

```bash
# Healthcheck backend
curl http://localhost:5000/health

# RÃ©cupÃ©rer les produits
curl http://localhost:5000/products | jq

# CrÃ©er un produit
curl -X POST http://localhost:5000/products \
  -H "Content-Type: application/json" \
  -d '{
    "name": "iPad Pro",
    "price": 1199.00,
    "category": "Tablettes"
  }'

# Test de charge (10 requÃªtes)
for i in {1..10}; do
  curl http://localhost:3000/api/products &
done
```

## ğŸ”§ Commandes utiles

### Gestion des services

```bash
# DÃ©marrer la stack
docker-compose up -d

# ArrÃªter la stack
docker-compose down

# RedÃ©marrer un service
docker-compose restart backend

# Voir les logs
docker-compose logs -f backend frontend

# Voir les stats
docker stats

# Reconstruire les images
docker-compose build --no-cache

# Nettoyer tout (âš ï¸ perte de donnÃ©es)
docker-compose down -v
```

### Debugging

```bash
# Entrer dans un container
docker-compose exec backend bash
docker-compose exec frontend sh

# VÃ©rifier la DB
docker-compose exec database psql -U user -d products
# SELECT * FROM products;

# Tester Elasticsearch
curl http://localhost:9200/_cluster/health?pretty

# VÃ©rifier Prometheus targets
curl http://localhost:9090/api/v1/targets | jq
```

### Monitoring

```bash
# Voir l'utilisation des ressources
docker-compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Ports}}"

# Logs d'un service avec timestamp
docker-compose logs -f --timestamps backend

# Top des containers par CPU/RAM
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"
```

## ğŸ“ Structure du projet

```
tp6-observability-stack/
â”œâ”€â”€ docker-compose.yml              # Stack complÃ¨te
â”œâ”€â”€ .env.example                    # Variables d'environnement
â”œâ”€â”€ init.sql                        # Initialisation PostgreSQL
â”œâ”€â”€ README.md                       # Ce fichier
â”‚
â”œâ”€â”€ frontend/                       # Microservice Node.js
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ server.js
â”‚   â””â”€â”€ public/index.html
â”‚
â”œâ”€â”€ backend/                        # Microservice Python
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ init_db.py
â”‚
â”œâ”€â”€ prometheus/                     # Config Prometheus
â”‚   â””â”€â”€ prometheus.yml
â”‚
â”œâ”€â”€ grafana/                        # Config Grafana
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ datasources.yml
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â””â”€â”€ dashboards.yml
â”‚   â””â”€â”€ dashboards/
â”‚
â””â”€â”€ logstash/                       # Config Logstash
    â””â”€â”€ pipeline.conf
```

## ğŸ” MÃ©triques disponibles

### Frontend (Node.js)
- `http_requests_total` - Total requÃªtes
- `http_request_duration_seconds` - Latence
- `frontend_errors_total` - Erreurs
- `nodejs_*` - MÃ©triques Node.js

### Backend (Python)
- `http_requests_total` - Total requÃªtes
- `http_request_duration_seconds` - Latence
- `database_queries_total` - RequÃªtes SQL
- `database_connection_pool` - Pool de connexions
- `flask_*` - MÃ©triques Flask

### PostgreSQL
- `pg_stat_database_*` - Stats DB
- `pg_stat_activity_*` - Connexions actives
- `pg_locks_*` - Locks
- Et 100+ autres mÃ©triques

## ğŸ› Troubleshooting

### ProblÃ¨me : Services ne dÃ©marrent pas
```bash
# VÃ©rifier les logs
docker-compose logs

# VÃ©rifier l'espace disque
df -h

# Nettoyer Docker
docker system prune -a --volumes
```

### ProblÃ¨me : Elasticsearch ne dÃ©marre pas (Out of Memory)
```bash
# Augmenter la mÃ©moire Docker (8GB min)
# Ou rÃ©duire ES_JAVA_OPTS dans .env
ES_JAVA_OPTS=-Xms256m -Xmx256m
```

### ProblÃ¨me : Backend ne peut pas se connecter Ã  la DB
```bash
# VÃ©rifier que PostgreSQL est UP
docker-compose ps database

# VÃ©rifier les logs
docker-compose logs database

# Attendre que la DB soit healthy
docker-compose up -d database
sleep 10
docker-compose up -d backend
```

### ProblÃ¨me : Pas de mÃ©triques dans Grafana
```bash
# VÃ©rifier les targets Prometheus
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health}'

# VÃ©rifier que les services exposent /metrics
curl http://localhost:3000/metrics
curl http://localhost:5000/metrics
```

## ğŸ“ ScÃ©narios de dÃ©monstration

### ScÃ©nario 1 : RequÃªte normale
1. Cliquer sur "RequÃªte Normale" dans le frontend
2. Observer dans Jaeger : trace complÃ¨te frontend â†’ backend â†’ DB
3. Observer dans Kibana : logs avec `trace_id` identique
4. Observer dans Grafana : pic de mÃ©trique `http_requests_total`

### ScÃ©nario 2 : DÃ©tection de latence
1. Cliquer sur "RequÃªte Lente"
2. Observer dans Jaeger : span de 5s pour `simulate_slow_operation`
3. CrÃ©er une alerte Prometheus si latence P95 > 2s

### ScÃ©nario 3 : Gestion d'erreur
1. Cliquer sur "GÃ©nÃ©rer Erreur"
2. Observer dans Kibana : log avec `level:ERROR`
3. Observer dans Jaeger : span marquÃ© avec tag `error:true`
4. Observer dans Grafana : augmentation de `frontend_errors_total`

### ScÃ©nario 4 : Test de charge
1. Cliquer sur "Charger 100 RequÃªtes"
2. Observer dans Grafana : pic de requÃªtes/sec
3. Observer mÃ©triques PostgreSQL : connexions actives
4. VÃ©rifier que les healthchecks restent verts

## ğŸ“š Ressources

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Elasticsearch Guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Jaeger Documentation](https://www.jaegertracing.io/docs/)
- [OpenTelemetry](https://opentelemetry.io/)

## ğŸ“„ Licence

MIT

---

**Bon monitoring ! ğŸ“ŠğŸ”ğŸ“ˆ**
